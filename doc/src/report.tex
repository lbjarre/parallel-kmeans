\documentclass{article}

\input{doc/src/preamble.tex}

\begin{document}

\begin{center}
  \textbf{
    \LARGE Parallel $k$-means Clustering \\
    \large SF2568 -- Parallel Computations for Large-Scale Problems \\
           \vspace{0.5em}
    \large \begin{tabular}{cc}
             Lukas Bjarre          & Gabriel Carrizo       \\
             \mail{lbjarre@kth.se} & \mail{carrizo@kth.se}
           \end{tabular} \\
  }
  \vspace{0.5em}
  \rule{\textwidth}{0.4pt}
\end{center}

\section{$k$-means clustering}
$k$-means clustering is a data clustering method
which clusters input data from the data set $\mathcal{X}$ into $k$ different classes.
The classes are represented by the class means $\mu_i$ and points are considered to be in a class $S_i$
if the squared distance to the class mean is the minimum
compared to the squared distance to the other class means.
Formally:
\[
      S_i = \{ \bm{x} \in \mathcal{X}
   :  ||\bm{x} - \bm{\mu}_i||^2 \leq ||\bm{x} - \bm{\mu}_j||^2
  ,\, \forall 1 \leq j \leq k \}
\]
A clustering method aims to find a selection of these classes
$\mathcal{S} = \{S_1, S_2, \ldots, S_k\}$
which divides the data points in some favorable way.
$k$-means finds the placement of the class means
by minimization of the summed squared distance of all class points to the class mean for all $k$ classes:
\[
  \mathcal{S}_{k\text{-means}}
  = \argmin_\mathcal{S} \sum_{i=1}^k \sum_{\bm{x} \in S_i} ||\bm{x} - \bm{\mu}_i||^2
\]
A common algorithm to find this is Lloyd's algorithm,
which iteratively classifies points according to current class means
and updates them with the average of all classified points until convergence.
\Cref{alg:lloyds} describes this procedure in pseudocode.

\begin{algorithm}[!ht]

  \newcommand{\cls}{\text{class}}
  \newcommand{\cnt}{\text{count}}
  \SetKw{Init}{initialize}

  \BlankLine
  \KwIn{Data points $\mathcal{X} = \{\bm{x}_1, \bm{x}_2, \ldots, \bm{x}_n\}$
        with $\bm{x}_i \in \mathbb{R}^d \; \forall \bm{x}_i \in \mathcal{X}$,
        number of clusters $k$}
  \KwOut{Class means $\bm{\mu}_1,\bm{\mu}_2,\ldots,\bm{\mu}_k$}
  \BlankLine

  \Init{} $\bar{\bm{\mu}}_i$ on random points in $\mathcal{X} \; \forall i = 1, \ldots, k$ \;
  \While{$\bm{\mu}_i \neq \bar{\bm{\mu}}_i \; \forall i = 1, \ldots, k$}{

    \For{$i=1, \ldots, k$}{

      $\bm{\mu}_i \gets \bar{\bm{\mu}}_i$ \;
      $\bar{\bm{\mu}}_i \gets 0$ \;

    }

    \ForAll{$\bm{x} \in \mathcal{X}$}{

      $\cls \gets \argmin_k ||\bm{x} - \bm{\mu}_k||^2$ \;
      $\cnt_\cls \gets \cnt_\cls + 1$ \;
      $\bar{\bm{\mu}}_\cls \gets \bar{\bm{\mu}}_\cls + \bm{x}$ \;
    
    }
    
    \For{$i=1,\ldots,k$}{
      $\bar{\bm{\mu}}_i \gets \frac{\bar{\bm{\mu}}_i}{\cnt_i}$ \;
      $\cnt_i \gets 0$ \;
    }
  
  }
  
  \Return{$\{ \bm{\mu}_1, \bm{\mu}_2, \ldots, \bm{\mu}_k \}$} \;

  \caption{Lloyd's algorithm for finding the $k$-means clustering class means.}
  \label{alg:lloyds}

\end{algorithm}

Lloyd's algorithm requires multiple loops for each iteration.
The most costly part is the classification of all data points,
which requires computation on all $d$ dimensions of the data set
for all $k$ classes over all $n$ points,
giving the algorithm a $\Theta(nkd)$ complexity.

\end{document}
